"""Definition of DAFs."""

from abc import ABCMeta, abstractmethod
from typing import Generic, TypeVar

from common import gen_rand

Measurement = TypeVar("Measurement")
AggParam = TypeVar("AggParam")
PublicShare = TypeVar("PublicShare")
InputShare = TypeVar("InputShare")
OutShare = TypeVar("OutShare")
AggShare = TypeVar("AggShare")
AggResult = TypeVar("AggResult")


class Daf(
        Generic[
            Measurement, AggParam, PublicShare, InputShare, OutShare, AggShare,
            AggResult
        ],
        metaclass=ABCMeta):
    """
    A Distributed Aggregation Function (DAF)

    Generic type parameters:
    Measurement -- the measurement type
    AggParam -- the aggregation parameter type
    PublicShare -- the public share type
    InputShare -- the input share type
    OutShare -- the output share type
    AggShare -- the aggregate share type
    AggResult -- the aggregate result type

    Attributes:
    ID -- algorithm identifier, a 32-bit integer
    SHARES -- the number of Aggregators
    NONCE_SIZE -- length of the nonce
    RAND_SIZE -- number of random bytes consumed by `shard()`
    """

    # Algorithm identifier for this DAF, a 32-bit integer.
    ID: int

    # The number of Aggregators.
    SHARES: int

    # Length of the nonce.
    NONCE_SIZE: int

    # Number of random bytes consumed by `shard()`.
    RAND_SIZE: int

    @abstractmethod
    def shard(
            self,
            measurement: Measurement,
            nonce: bytes,
            rand: bytes) -> tuple[PublicShare, list[InputShare]]:
        """
        Shard a measurement into a public share and a sequence of input
        shares, one for each Aggregator. This method is run by the Client.

        Pre-conditions:

            - `len(nonce) == Daf.NONCE_SIZE`
            - `len(rand) == Daf.RAND_SIZE`
        """
        pass

    @abstractmethod
    def is_valid(
            self,
            agg_param: AggParam,
            previous_agg_params: set[AggParam]) -> bool:
        """
        Check if `agg_param` is valid for use with an input share that has
        previously been used with all `previous_agg_params`.
        """
        pass

    @abstractmethod
    def prep(
            self,
            agg_id: int,
            agg_param: AggParam,
            nonce: bytes,
            public_share: PublicShare,
            input_share: InputShare) -> OutShare:
        """
        Prepare an input share for aggregation. This algorithm takes in the
        public share and one of the input shares generated by the Client. It
        also takes the Aggregator's ID (a unique integer in range `[0, SHARES)`
        corresponding to the index of `input_share` in the Client's output),
        and an aggregation parameter and returns the corresponding output
        share.

        Pre-conditions:

            - `agg_id in range(0, Daf.SHARES)`
            - `len(nonce) == Daf.NONCE_SIZE`
        """
        pass

    @abstractmethod
    def aggregate(
            self,
            agg_param: AggParam,
            out_shares: list[OutShare]) -> AggShare:
        """
        Merge a list of output shares into an aggregate share, encoded as a
        byte string. This is called by an Aggregator after recovering a batch
        of output shares.
        """
        pass

    @abstractmethod
    def unshard(
            self,
            agg_param: AggParam,
            agg_shares: list[AggShare],
            num_measurements: int) -> AggResult:
        """
        Unshard the aggregate shares (encoded as byte strings) and compute the
        aggregate result. This is called by the Collector.
        """
        pass


# NOTE: This function is excerpted in the document, as the figure
# {{run-daf}}. Its width should be limited to 69 columns to avoid
# warnings from xml2rfc.
# ===================================================================
def run_daf(
        daf: Daf[
            Measurement,
            AggParam,
            PublicShare,
            InputShare,
            OutShare,
            AggShare,
            AggResult,
        ],
        agg_param: AggParam,
        measurements: list[Measurement],
        nonces: list[bytes]) -> AggResult:
    """
    Run a DAF on a list of measurements.

    Pre-conditions:

        - `type(agg_param) == daf.AggParam`
        - `type(measurement) == daf.Measurement` for each
          `measurement` in `measurements`
        - `len(nonce) == daf.NONCE_SIZE` for each `nonce` in `nonces`
        - `len(nonces) == len(measurements)`
    """
    if any(len(nonce) != daf.NONCE_SIZE for nonce in nonces):
        raise ValueError("incorrect nonce size")
    if len(nonces) != len(measurements):
        raise ValueError(
            "measurements and nonces lists have different lengths"
        )

    out_shares: list[list[OutShare]]
    out_shares = [[] for j in range(daf.SHARES)]
    for (measurement, nonce) in zip(measurements, nonces):
        # Each Client shards its measurement into input shares and
        # distributes them among the Aggregators.
        rand = gen_rand(daf.RAND_SIZE)
        (public_share, input_shares) = \
            daf.shard(measurement, nonce, rand)

        # Each Aggregator prepares its input share for aggregation.
        for j in range(daf.SHARES):
            out_shares[j].append(
                daf.prep(j, agg_param, nonce,
                         public_share, input_shares[j]))

    # Each Aggregator aggregates its output shares into an aggregate
    # share and sends it to the Collector.
    agg_shares = []
    for j in range(daf.SHARES):
        agg_share_j = daf.aggregate(agg_param,
                                    out_shares[j])
        agg_shares.append(agg_share_j)

    # Collector unshards the aggregate result.
    num_measurements = len(measurements)
    agg_result = daf.unshard(agg_param, agg_shares,
                             num_measurements)
    return agg_result
